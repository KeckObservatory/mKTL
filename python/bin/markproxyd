#! /usr/bin/env kpython3

# This is a mKTL proxy front-end. It should be configuration driven, and be
# willing to invoke as many subprocesses as necessary in order to provide
# mKTL access to any number of different proxied store components.

import atexit
import itertools
import json
import os
import socket
import subprocess
import sys
import tempfile
import threading
import time
import zmq

import mKTL

zmq_context = zmq.Context()


def main():

    ### This is a quick+dirty approach to running up a KTL/mKTL proxy. The real
    ### proxy daemon should be driven by a ConfigParser configuration file,
    ### which defines the name of the store, the proxy subprocess call that
    ### should be invoked, and any optional arguments.

    if len(sys.argv) < 2:
        raise ValueError('no service specified on the command line')

    names = sys.argv[1:]

    pub = mKTL.Protocol.Publish.Server()
    req = RequestServer(port=10112)
    ##req = RequestServer(port=10111)

    discovery = mKTL.Protocol.Discover.DirectServer(request_port=req.port)

    # Try once to locate any running guide processes and notify them about
    # our unique local configuration. Further discovery of running daemons,
    # as might need to occur after the guide restarts, is their responsibility.

    main.guides = mKTL.Protocol.Discover.search(wait=True)

    # PUB and REP messages can flow directly from the subprocess out to the
    # external interface without any additional handling; inbound requests
    # on the external interface have to be parsed to know which subprocess
    # should handle the request.

    # The handling here only allows for unique store names associated with
    # a subprocess/relay; does this need to allow for the possibility of
    # multiple subprocesses associated with a single store name?

    for name in names:
        main.subs[name] = ProxySubprocess(name, req, pub)


    while True:

        try:
            main.shutdown.wait(30)
        except (KeyboardInterrupt, SystemExit):
            break


main.subs = dict()
main.shutdown = threading.Event()



class InternalRequest:
    ''' The :class:`InternalRequest` provides a very thin wrapper around a
        :class:`threading.Event` that can be used to signal the internal
        caller that the request has been handled. It also provides a vehicle
        for the response to be passed to the caller.
    '''

    def __init__(self):
        self.event = threading.Event()


    def complete(self, response):
        ''' If a response to an internal request arrives the
            :class:`RequestRelay` instance will check whether the response
            belongs to an internal request, and if it does, call
            :func:`complete` to indicate the response has arrived.
        '''

        self.response = response
        self.event.set()


    def wait(self):
        ''' The invoker of the :class:`InternalRequest` will call :func:`wait`
            to block until the request has been handled.
        '''

        self.event.wait()
        return self.response


# end of class InternalRequest



class ProxySubprocess:
    ''' The :class:`ProxySubprocess` contains all the elements associated
        with the subprocess handling proxied requests. Those components are:

          * The subprocess itself.

          * ZeroMQ sockets to communicate with the subprocess.

          * A :class:`RequestRelay` instance to receive requests from the
            proxy's external interface and redirect them to the subprocess;
            likewise, to redirect responses back to the external interface.

          * A :class:`PublishRelay` instance that acts as a direct conduit
            for broadcasts published by the subprocess, relaying them
            unchanged to the external interface.

        The subprocess will be restarted if it stops running for any reason.
        The ZeroMQ sockets created here get re-used if that occurs.
    '''

    def __init__(self, name, req_server, pub_server):

        socketdir = tempfile.mkdtemp()
        atexit.register(cleanup_sockets, socketdir)

        req_address = os.path.join(socketdir, 'req')
        pub_address = os.path.join(socketdir, 'pub')

        req_address = 'ipc://' + req_address
        pub_address = 'ipc://' + pub_address

        self.name = name
        self.pub_address = pub_address
        self.req_address = req_address

        self.pub = PublishRelay(pub_address, pub_server)
        self.req = RequestRelay(req_address, req_server)

        # The subprocess should be instantiated last so that the relays are
        # intact and ready to be accessed.

        self.subprocess = None

        self.thread = threading.Thread(target=self.run)
        self.thread.daemon = True
        self.thread.start()


    def run(self):

        self.subprocess = self.startSubprocess()

        while True:
            try:
                exitcode = self.subprocess.wait(30)
            except subprocess.TimeoutExpired:
                continue

            # Process died. Go back to the top of the loop after some artificial
            # delay to give things a chance to recover.

            ### This should log appropriately, and probably check for some sort
            ### of broader indicator that we should no longer be running.

            print('Proxy subprocess died for ' + self.name)
            time.sleep(10)
            self.subprocess = self.startSubprocess()


    def reconfig(self):

        request = dict()
        request['request'] = 'CONFIG'
        request['name'] = self.name

        new_config = self.req.internal_request(request)

        ### Error handling goes here. If there is an error, or there is
        ### no configuration data, log the error, at a minimum.

        try:
            new_config['data']
        except KeyError:
            error = new_config['error']
            print(error['debug'])
            exception = RuntimeError('request for configuration data failed')
            exception.add_note('Original traceback:')
            exception.add_note(error['debug'])
            raise exception


        # Pass the new configuration upstream so it can be cached for future
        # queries. Reaching in here feels a bit like too much close coupling;
        # it'd be nice if we could pass a request "up" through a more normal
        # channel instead of calling a "private" method in this fashion.

        # Using an mKTL client connection to ourself is one possible approach,
        # but that still involves reaching into self.req.external to know what
        # address and port to contact.

        local_provenance = dict()
        local_provenance['stratum'] = 0
        local_provenance['hostname'] = self.req.external.hostname
        local_provenance['req'] = self.req.external.port
        local_provenance['pub'] = self.pub.external.port

        provenance = list()
        provenance.append(local_provenance)

        request['data'] = new_config['data']
        request['data']['provenance'] = provenance
        self.req.external.req_config(None, request)

        # Distribute what we know to any established guides.

        for address,port in main.guides:
            send_configuration(address, port)



    def startSubprocess(self):

        arguments = list()
        arguments.append('./marksub')
        arguments.append(self.req_address)
        arguments.append(self.pub_address)
        arguments.append(self.name)

        pipe = subprocess.PIPE
        ##proxysub = subprocess.Popen(arguments, stdout=pipe, stderr=pipe)
        proxysub = subprocess.Popen(arguments)

        self.reconfig()
        return proxysub


# end of class ProxySubprocess



class PublishRelay:
    ''' A :class:`PublishRelay` instance acts as a direct conduit for broadcasts
        published by the subprocess, relaying them unchanged to the external
        interface.

        This is accomplished very efficiently by :func:`zmq.proxy`, which means
        we are not inspecting or modifying any broadcasts as part of the relay.
    '''

    def __init__(self, internal, external):

        self.external = external

        self.internal = internal
        self.internal_socket = zmq_context.socket(zmq.SUB)
        self.internal_socket.connect(internal)
        self.internal_socket.setsockopt(zmq.SUBSCRIBE, b'')

        # Since there's no additional handling with published messages we can
        # leverage a direct ZeroMQ proxy routine. We still have to encapsulate
        # it in a background thread, as the zmq.proxy() call never returns.

        proxy_args = (self.external.socket, self.internal_socket)
        thread = threading.Thread(target=zmq.proxy, args=proxy_args)
        thread.daemon = True
        thread.start()


# end of class PublishRelay



class RequestRelay:
    ''' A :class:`RequestRelay` instance receives requests from the external
        interface and redirects them to the subprocess; likewise, it redirects
        responses back to the external interface.

        Requests have locally unique internal identifier numbers, which implies
        the need to remap external request numbers to something locally unique,
        and back again for the response.
    '''

    req_id_min = 0
    req_id_max = 0xFFFFFFFF

    def __init__(self, internal, external):

        self.req_id_lock = threading.Lock()
        self.req_id_reset()
        self.external = external

        self.internal = internal
        self.internal_socket = zmq_context.socket(zmq.DEALER)
        self.internal_socket.bind(internal)
        self.internal_socket_lock = threading.Lock()
        self.internal_socket_poller = zmq.Poller()
        self.internal_socket_poller.register(self.internal_socket, zmq.POLLOUT)

        self.active_requests = dict()

        self.thread = threading.Thread(target=self.run)
        self.thread.daemon = True
        self.thread.start()


    def external_request(self, ident, request):
        ''' Relay a request from the external interface to our subprocess for
            proper handling.
        '''

        # Preserve the original id and ident for the future response. The
        # request id must be remapped to a value that is locally unique;
        # otherwise, two clients using the same id would cause problems with
        # our internal tracking of the request. It is perfectly reasonable for
        # two clients to issue requests starting with 1 as their locally unique
        # identifier, and we must be able to distinguish between them.

        original_id = request['id']
        req_id = self.req_id_next()
        request['id'] = req_id

        self.active_requests[req_id] = (original_id, ident)

        request = json.dumps(request)
        request = request.encode()
        self.internal_socket_lock.acquire()
        self.internal_socket.send(request)
        self.internal_socket_lock.release()


    def internal_request(self, request):
        ''' Issue an internal request to a subprocess, via the subprocess
            request socket. Return the response to the caller.
        '''

        req_id = self.req_id_next()
        request['id'] = req_id

        internal = InternalRequest()
        self.active_requests[req_id] = (internal, None)

        request = json.dumps(request)
        request = request.encode()

        # Not entirely sure why polling here is required, but it is. The poll
        # call always returns, sometimes at the timeout, sometimes sooner,
        # reporting that the socket is ready for output.

        sockets = self.internal_socket_poller.poll(200)

        self.internal_socket_lock.acquire()
        self.internal_socket.send(request)
        self.internal_socket_lock.release()

        response = internal.wait()
        del self.active_requests[req_id]

        return response


    def req_id_next(self):
        ''' Return the next request identification number for subroutines to
            use when relaying a request. Internal requests are assigned a
            number; external requests have their original identifier mapped to
            a locally unique identifier.
        '''

        self.req_id_lock.acquire()
        req_id = next(self.req_id)

        if req_id >= self.req_id_max:
            self.req_id_reset()

            if req_id > self.req_id_max:
                # This shouldn't happen, but here we are...
                req_id = self.req_id_min
                next(self.req_id)

        self.req_id_lock.release()
        return req_id


    def req_id_reset(self):
        ''' Reset the publication identification number to the minimum value.
        '''

        self.req_id = itertools.count(self.req_id_min)


    def run(self):

        ### This needs to watch for errors and possibly flag that the
        ### subprocess needs to be restarted.

        # Messages outbound from the subprocess should be relayed directly
        # to the external socket.

        poller = zmq.Poller()
        poller.register(self.internal_socket, zmq.POLLIN)

        while True:
            sockets = poller.poll(10000)
            for active, flag in sockets:
                if self.internal_socket == active:
                    self.internal_socket_lock.acquire()
                    response = self.internal_socket.recv()
                    self.internal_socket_lock.release()

                    ### This assumes the response is JSON. This won't work
                    ### in the bulk data case.

                    response_dict = json.loads(response)
                    response_id = response_dict['id']

                    (original_id, ident) = self.active_requests[response_id]

                    if ident is None:
                        # This is an internal request. We aren't handling ACK
                        # responses, just real ones.
                        response_type = response_dict['message']
                        if response_type == 'REP':
                            request = original_id
                            request.complete(response_dict)
                        continue

                    # Any response that is not strictly internal is destined
                    # for the external interface. The original request id was
                    # remapped to a locally unique value; the client is
                    # expecting to see the original id in the response.

                    response_dict['id'] = original_id
                    response = json.dumps(response_dict)
                    response = response.encode()

                    ### The removal of the id -> ident mapping would need to be
                    ### more intelligent to handle the bulk data case.

                    if response_dict['message'] != 'ACK':
                        del self.active_requests[response_id]

                    self.external.send(ident, response)


# end of class RequestRelay



class RequestServer(mKTL.Protocol.Request.Server):
    ''' Subclass the generic :class:mKTL.Protocol.Request.Server`, adding
        additional logic to locally handle CONFIG requests, and to determine
        which proxied subprocess should receive the incoming request.
    '''

    def req_config(self, ident, request):
        ''' Return a configuration block for the requested store name. The
            configuration blocks are stored locally every time a subprocess
            starts running.
        '''

        try:
            request_name = request['name']
        except KeyError:
            raise KeyError('CONFIG request must specify a name')


        try:
            data = request['data']
        except KeyError:
            pass
        else:
            # The presence of a data block implies a 'set' operation.
            # Cache the information, and return an empty dict to indicate
            # the requested operation is complete.

            mKTL.Config.Cache.add(request_name, data)
            return dict()

        try:
            data = mKTL.Config.Cache.get(request_name)
        except KeyError:
            raise KeyError('no local configuration for ' + repr(request_name))

        payload = mKTL.Config.Cache.get(request_name)
        return payload


    def req_handler(self, socket, lock, ident, request):
        ''' Inspect the incoming request type and decide how a response
            will be generated. Interactive requests are handled exclusively
            by the subprocess, including the ACK of the request; other request
            types are handled immediately via subroutines called here.
        '''

        try:
            type = request['request']
        except KeyError:
            self.req_ack(socket, lock, ident, request)
            raise KeyError("invalid request JSON, 'request' not set")

        try:
            name = request['name']
        except KeyError:
            if type != 'HASH':
                self.req_ack(socket, lock, ident, request)
                raise KeyError("invalid request JSON, 'name' not set")

        if type == 'GET' or type == 'SET':
            # The subprocess will ACK these requests, as well as issue
            # appropriate responses once the request is handled; returning
            # None from this routine will indicates no immediate response
            # should be issued.

            self.req_relay(ident, request)
            payload = None

        else:
            # Configuration requests are handled locally. ACK the request,
            # though the response will arrive quickly enough that an ACK
            # may not strictly be necessary.

            self.req_ack(socket, lock, ident, request)
            if type == 'HASH':
                payload = self.req_hash(ident, request)
            elif type == 'CONFIG':
                payload = self.req_config(ident, request)
            else:
                raise ValueError('unhandled request type: ' + type)

        return payload


    def req_hash(self, ident, request):
        ''' Return the hash of a configuration block for a requested store name,
            or all hashes for all locally known stores.
        '''

        try:
            name = request['name']
        except KeyError:
            name = None

        cached = mKTL.Config.Hash.get(name)
        return cached


    def req_relay(self, ident, request):
        ''' Relay the incoming GET or SET request to the correct subprocess.
            The :class:`RequestRelay` instance associated with that subprocess
            handles the communication in both directions.
        '''

        # The request could be for one of any of the stores associated with
        # this proxy. We have to know the store name to know which subprocess
        # to send it to.

        name = request['name']
        store = name.split('.', 1)[0]

        try:
            sub = main.subs[store]
        except KeyError:
            raise KeyError('no local store for ' + repr(store))

        sub.req.external_request(ident, request)


# end of class RequestServer



def cleanup_sockets(socketdir):
    sockets = os.listdir(socketdir)

    for socket in sockets:
        os.remove(os.path.join(socketdir, socket))

    os.rmdir(socketdir)



def send_configuration(address, port):
    ''' Send a CONFIG request providing locally known data to the daemon
        running on the specified address and port.
    '''

    names = mKTL.Config.Cache.list()

    request = dict()
    request['request'] = 'CONFIG'

    for name in names:
        request['name'] = name
        request['data'] = mKTL.Config.Cache.get(name)

        try:
            mKTL.Protocol.Request.send(request, address, port)
        except zmq.error.ZMQError:
            pass



if __name__ == '__main__':
    main()

# vim: set expandtab tabstop=8 softtabstop=4 shiftwidth=4 autoindent:
